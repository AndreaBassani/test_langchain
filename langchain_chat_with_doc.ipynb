{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain.prompts as prompts\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts.chat import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import io\n",
    "import requests\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['PINECONE_API_KEY'] = ''\n",
    "os.environ['ENVIROMENT_KEY'] = ''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_pinecone():\n",
    "    PINECONE_API_KEY = os.environ['PINECONE_API_KEY']\n",
    "    ENVIROMENT_KEY = os.environ['ENVIROMENT_KEY']\n",
    "    pinecone.init(api_key=PINECONE_API_KEY, environment=ENVIROMENT_KEY)\n",
    "\n",
    "def create_pinecone_index(pinecone_index_name):\n",
    "    initialise_pinecone()\n",
    "    pinecone.create_index(name = pinecone_index_name, dimension = 1536, metric = 'cosine')\n",
    "\n",
    "def describe_index(pinecone_index_name):\n",
    "    initialise_pinecone()\n",
    "    index = pinecone.Index(pinecone_index_name)\n",
    "    return index.describe_index_stats()\n",
    "\n",
    "def set_docsearch(pinecone_index_name, namespace, data):\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key = os.environ['OPENAI_API_KEY'])\n",
    "    initialise_pinecone()\n",
    "    if pinecone_index_name in pinecone.list_indexes():\n",
    "        list_of_namespaces = list(describe_index(pinecone_index_name)['namespaces'].keys())\n",
    "        if namespace in list_of_namespaces:\n",
    "            return Pinecone.from_existing_index(index_name=pinecone_index_name, embedding=embeddings, namespace=namespace)\n",
    "        else:\n",
    "            index = pinecone.Index(pinecone_index_name)\n",
    "            p = Pinecone(index = index, embedding_function = embeddings, text_key = 'text')\n",
    "            return p.add_texts(texts=data, embeddings = embeddings, namespace=namespace, index_name=pinecone_index_name)\n",
    "            \n",
    "    else:\n",
    "        create_pinecone_index(pinecone_index_name)\n",
    "        return Pinecone.from_texts(data, embedding=embeddings, index_name=pinecone_index_name, namespace = namespace)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import pdf and split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    return text_splitter\n",
    "\n",
    "def parse_online_pdf(url, chunk_size=400, chunk_overlap=20):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Windows; Windows x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36'}\n",
    "    response = requests.get(url=url, headers=headers, timeout=120)\n",
    "    on_fly_mem_obj = io.BytesIO(response.content)\n",
    "    pdfReader = PdfReader(on_fly_mem_obj)\n",
    "    full_text = \"\"\n",
    "    for i, page in enumerate(pdfReader.pages):\n",
    "        full_text += page.extract_text()\n",
    "    chuncks = text_splitter(chunk_size, chunk_overlap).split_text(full_text)\n",
    "    return chuncks "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt engeneering and chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = prompts.PromptTemplate(\n",
    "    input_variables=[\"question\", \"context_str\", \"length\"],\n",
    "    template=\"\"\"Write an answer ({length}) \n",
    "    for the question below solely based on the provided context. \n",
    "    If the context provides insufficient information,\n",
    "    reply 'I cannot answer'. \n",
    "    For each sentence in your answer, indicate which sources most support it\n",
    "    via valid citation markers at the end of sentences, like (Example2012).\n",
    "    Answer in an unbiased and scholarly tone. Make clear what is your opinion.\n",
    "    Use Markdown for formatting code or text, and try to use direct quotes to support arguments.\\n\\n\n",
    "    {context_str}\\n\n",
    "    Question: {question}\\n\n",
    "    Answer: \"\"\",\n",
    ")\n",
    "\n",
    "def make_chain(prompt, llm):\n",
    "    if type(llm) == ChatOpenAI:\n",
    "        system_message_prompt = SystemMessage(\n",
    "            content=\"\"\"You are a scholarly researcher that answers in an unbiased, scholarly tone.\n",
    "            You sometimes refuse to answer if there is insufficient information.\"\"\",\n",
    "        )\n",
    "        human_message_prompt = HumanMessagePromptTemplate(prompt=prompt)\n",
    "        prompt = ChatPromptTemplate.from_messages(\n",
    "            [system_message_prompt, human_message_prompt]\n",
    "        )\n",
    "    return LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise the LLM model and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
    "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1150988/JROC_report_recommendations_and_actions_paper_April_2023.pdf'\n",
    "pinecone_index_name = 'booktoavatar'\n",
    "namespace = 'test_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'OpenAIEmbeddings' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[39m=\u001b[39m parse_online_pdf(url)\n\u001b[1;32m----> 2\u001b[0m doc_search \u001b[39m=\u001b[39m set_docsearch(pinecone_index_name, namespace, data)\n\u001b[0;32m      3\u001b[0m docs \u001b[39m=\u001b[39m doc_search\u001b[39m.\u001b[39msimilarity_search(query, namespace\u001b[39m=\u001b[39mnamespace)\n\u001b[0;32m      4\u001b[0m \u001b[39m# parse the output\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[80], line 25\u001b[0m, in \u001b[0;36mset_docsearch\u001b[1;34m(pinecone_index_name, namespace, data)\u001b[0m\n\u001b[0;32m     23\u001b[0m         index \u001b[39m=\u001b[39m pinecone\u001b[39m.\u001b[39mIndex(pinecone_index_name)\n\u001b[0;32m     24\u001b[0m         p \u001b[39m=\u001b[39m Pinecone(index \u001b[39m=\u001b[39m index, embedding_function \u001b[39m=\u001b[39m embeddings, text_key \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m p\u001b[39m.\u001b[39;49madd_texts(texts\u001b[39m=\u001b[39;49mdata, embeddings \u001b[39m=\u001b[39;49m embeddings, namespace\u001b[39m=\u001b[39;49mnamespace, index_name\u001b[39m=\u001b[39;49mpinecone_index_name)\n\u001b[0;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     create_pinecone_index(pinecone_index_name)\n",
      "File \u001b[1;32mc:\\Users\\bassa\\miniconda3\\envs\\vercel_test\\lib\\site-packages\\langchain\\vectorstores\\pinecone.py:84\u001b[0m, in \u001b[0;36mPinecone.add_texts\u001b[1;34m(self, texts, metadatas, ids, namespace, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m     82\u001b[0m ids \u001b[39m=\u001b[39m ids \u001b[39mor\u001b[39;00m [\u001b[39mstr\u001b[39m(uuid\u001b[39m.\u001b[39muuid4()) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m texts]\n\u001b[0;32m     83\u001b[0m \u001b[39mfor\u001b[39;00m i, text \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(texts):\n\u001b[1;32m---> 84\u001b[0m     embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_function(text)\n\u001b[0;32m     85\u001b[0m     metadata \u001b[39m=\u001b[39m metadatas[i] \u001b[39mif\u001b[39;00m metadatas \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m     86\u001b[0m     metadata[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_key] \u001b[39m=\u001b[39m text\n",
      "\u001b[1;31mTypeError\u001b[0m: 'OpenAIEmbeddings' object is not callable"
     ]
    }
   ],
   "source": [
    "data = parse_online_pdf(url)\n",
    "doc_search = set_docsearch(pinecone_index_name, namespace, data)\n",
    "docs = doc_search.similarity_search(query, namespace=namespace)\n",
    "# parse the output\n",
    "output = qa_chain.run(question=query, context_str=docs, length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context includes information on the DIMPACT initiative, which aims to help media compan\n",
      "ies reduce the carbon footprint of their digital value chain by providing a tool that models the env\n",
      "ironmental impacts of digital products and services. The initiative involves the DIMPACT Expert Advi\n",
      "sory Panel in developing the methodology, and provides a summary of data input requirements and part\n",
      "icipant results from modeling runs. The carbon intensity of electricity for end-user devices varies \n",
      "by module, and the tool allows users to calculate this within the publishing module by listing the p\n",
      "roportions of their views per country. There is also information on the standby power and carbon int\n",
      "ensity of various end-user devices such as smartphones, laptops, and desktops. (DIMPACT Methodology,\n",
      " October 2022; Carbon Trust White Paper, BBC White Paper)\n"
     ]
    }
   ],
   "source": [
    "chars=100\n",
    "for i in range(0, len(output), chars):\n",
    "    print(output[i:i+chars])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vercel_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
